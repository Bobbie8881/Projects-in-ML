{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVFTG0FjBykl"
      },
      "source": [
        "<p align=\"center\"><img width=\"50%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/aimodshare_banner.jpg\" /></p>\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXxGTgJz152A"
      },
      "source": [
        "## Assignment 3: Text Classification Using the Stanford SST Sentiment Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GitHub link: https://github.com/Bobbie8881/Projects-in-ML"
      ],
      "metadata": {
        "id": "7JOeE5bPF2lg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gSrVJwp3E9H"
      },
      "source": [
        "## Get data in and set up X_train, X_test, y_train objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PLTIaMB3ChSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "934b39c1-13e2-4824-f8ac-62a05b0ae302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting aimodelshare==0.0.189\n",
            "  Downloading aimodelshare-0.0.189-py3-none-any.whl (967 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m967.8/967.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.9.2\n",
            "  Downloading tensorflow-2.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf2onnx\n",
            "  Downloading tf2onnx-1.14.0-py3-none-any.whl (451 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.2/451.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2022.10.31)\n",
            "Collecting onnxconverter-common>=1.7.0\n",
            "  Downloading onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3==1.26.69\n",
            "  Downloading boto3-1.26.69-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (5.9.4)\n",
            "Collecting scipy==1.7.0\n",
            "  Downloading scipy-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime>=1.7.0\n",
            "  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker==5.0.0\n",
            "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (2.0.0+cu118)\n",
            "Collecting PyJWT>=2.4.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Collecting keras2onnx>=1.7.0\n",
            "  Downloading keras2onnx-1.7.0-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.3/96.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shortuuid>=1.0.8\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting protobuf==3.19.6\n",
            "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pympler==0.9\n",
            "  Downloading Pympler-0.9.tar.gz (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.4/178.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.6.3)\n",
            "Requirement already satisfied: seaborn>=0.11.2 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (0.12.2)\n",
            "Collecting botocore==1.29.82\n",
            "  Downloading botocore-1.29.82-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.2.1\n",
            "  Downloading scikit_learn-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxmltools>=1.6.1\n",
            "  Downloading onnxmltools-1.11.2-py2.py3-none-any.whl (322 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydot==1.3.0\n",
            "  Downloading pydot-1.3.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pathlib>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from aimodelshare==0.0.189) (1.0.1)\n",
            "Collecting onnx==1.12.0\n",
            "  Downloading onnx-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-resources==5.10.0\n",
            "  Downloading importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
            "Collecting skl2onnx>=1.14.0\n",
            "  Downloading skl2onnx-1.14.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.0/294.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (0.40.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->aimodelshare==0.0.189) (1.16.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.9/dist-packages (from botocore==1.29.82->aimodelshare==0.0.189) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore==1.29.82->aimodelshare==0.0.189) (2.8.2)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker==5.0.0->aimodelshare==0.0.189) (1.5.1)\n",
            "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/lib/python3.9/dist-packages (from docker==5.0.0->aimodelshare==0.0.189) (2.27.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources==5.10.0->aimodelshare==0.0.189) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx==1.12.0->aimodelshare==0.0.189) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx==1.12.0->aimodelshare==0.0.189) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.9/dist-packages (from pydot==1.3.0->aimodelshare==0.0.189) (3.0.9)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.1->aimodelshare==0.0.189) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (23.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.32.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.53.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (3.3.0)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (67.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.9.2->aimodelshare==0.0.189) (1.14.1)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.11.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.9/dist-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.9/dist-packages (from seaborn>=0.11.2->aimodelshare==0.0.189) (1.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->aimodelshare==0.0.189) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->aimodelshare==0.0.189) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->aimodelshare==0.0.189) (3.25.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.11.2->aimodelshare==0.0.189) (8.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25->seaborn>=0.11.2->aimodelshare==0.0.189) (2022.7.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests!=2.18.0,>=2.14.2->docker==5.0.0->aimodelshare==0.0.189) (2022.12.7)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.4.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->aimodelshare==0.0.189) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime>=1.7.0->aimodelshare==0.0.189) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (6.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->aimodelshare==0.0.189) (3.2.2)\n",
            "Building wheels for collected packages: Pympler, wget, fire\n",
            "  Building wheel for Pympler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pympler: filename=Pympler-0.9-py3-none-any.whl size=164822 sha256=e275680f9ee28dd3028015906ef0ccf57cb6383afad2aa3129c89f11403af206\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/84/9a/0e7aa69b52bc9dc13ea25679c8d8f1dc11bc5b5289db23d9f3\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=b368faceb007b59cfb355a828c8945eb6f1ab5ed655940c1c5e3c411e0c7e195\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=a3faed4601090015cf61ddd680dcfa2a216a881804b094bd869ddc4238fb6166\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built Pympler wget fire\n",
            "Installing collected packages: wget, Pympler, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, shortuuid, scipy, PyJWT, pydot, protobuf, keras-preprocessing, jmespath, importlib-resources, humanfriendly, fire, scikit-learn, onnx, docker, coloredlogs, botocore, tf2onnx, s3transfer, onnxruntime, onnxconverter-common, google-auth-oauthlib, tensorboard, skl2onnx, keras2onnx, boto3, tensorflow, onnxmltools, aimodelshare\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.0\n",
            "    Uninstalling tensorboard-data-server-0.7.0:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 1.4.2\n",
            "    Uninstalling pydot-1.4.2:\n",
            "      Successfully uninstalled pydot-1.4.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.12.0\n",
            "    Uninstalling importlib-resources-5.12.0:\n",
            "      Successfully uninstalled importlib-resources-5.12.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.1\n",
            "    Uninstalling tensorboard-2.12.1:\n",
            "      Successfully uninstalled tensorboard-2.12.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyJWT-2.6.0 Pympler-0.9 aimodelshare-0.0.189 boto3-1.26.69 botocore-1.29.82 coloredlogs-15.0.1 docker-5.0.0 fire-0.5.0 flatbuffers-1.12 google-auth-oauthlib-0.4.6 humanfriendly-10.0 importlib-resources-5.10.0 jmespath-1.0.1 keras-2.9.0 keras-preprocessing-1.1.2 keras2onnx-1.7.0 onnx-1.12.0 onnxconverter-common-1.13.0 onnxmltools-1.11.2 onnxruntime-1.14.1 protobuf-3.19.6 pydot-1.3.0 s3transfer-0.6.0 scikit-learn-1.2.1 scipy-1.7.0 shortuuid-1.0.11 skl2onnx-1.14.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorflow-2.9.2 tensorflow-estimator-2.9.0 tf2onnx-1.14.0 wget-3.2\n"
          ]
        }
      ],
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare==0.0.189"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in Data"
      ],
      "metadata": {
        "id": "9fcZ44uRFQt_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3PiJXBhC5y-",
        "outputId": "b5268627-d744-460c-f55e-fff6c5b5e064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [=============================================>   ]\n",
            "\n",
            "Data downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Get competition data\n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT0qFCZFNzHq",
        "outputId": "28015c8f-f4af-4ad1-ff91-b2179a762d3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    The Rock is destined to be the 21st Century 's...\n",
              "1    The gorgeously elaborate continuation of `` Th...\n",
              "2    Singer/composer Bryan Adams contributes a slew...\n",
              "3                 Yet the act is still charming here .\n",
              "4    Whether or not you 're enlightened by any of D...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Set up X_train, X_test, and y_train_labels objects\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
        "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
        "\n",
        "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
        "\n",
        "# ohe encode Y data\n",
        "y_train = pd.get_dummies(y_train_labels)\n",
        "\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain."
      ],
      "metadata": {
        "id": "rjpFs0ALGrTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_IqRBW6Jeyg",
        "outputId": "4be0815a-7e73-4fd5-e6d1-6689adff9a63"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sB4sSpWJobS",
        "outputId": "a6a53c59-5b76-4a59-fd6f-41008e0a619b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWdLW3mJJ7yL",
        "outputId": "d5d94ff5-ce86-4ba5-fb4c-502e6e9bc480"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Positive\n",
              "1       Positive\n",
              "2       Positive\n",
              "3       Positive\n",
              "4       Positive\n",
              "          ...   \n",
              "6915    Negative\n",
              "6916    Negative\n",
              "6917    Positive\n",
              "6918    Negative\n",
              "6919    Negative\n",
              "Name: label, Length: 6920, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sst2_competition_data is a dataset that contains movie reviews along with their corresponding sentiment labels, either \"positive\" or \"negative\". The dataset consists of 6,920 training samples and 1821 test samples.\n",
        "\n",
        "Building a predictive model using this dataset could be pratically useful for businesses in the entertainment industry, such as movie production companies, streaming services, and movie review websites. With the ability to accurately predict the sentiment of movie reviews, such businesses could obtain valuable insights regarding the public's perception of their movies, which could aid in refining their marketing strategies and making well-informed decisions pertaining to the production, distribution, and promotion of their movies.\n",
        "\n",
        "For example, by examining the pre-release evaluations of a new film, the movie production company may apply a predictive model trained on this information to forecast the likelihood that the film will be successful. If the model predicts a negative sentiment, the studio may decide to alter the film or its marketing plan in an effort to increase the likelihood of success. On the other hand, if the model forecasts a favorable mood, the business can utilize that data to customize its marketing strategy to increase awareness and ticket sales."
      ],
      "metadata": {
        "id": "8DvuPaJUGucL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzPoXPj3V7u"
      },
      "source": [
        "## Preprocess data using keras tokenizer / Write and Save Preprocessor function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16QV9Y9TC3B3",
        "outputId": "ca278d00-4596-4be7-b078-4125827c460e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 40)\n",
            "(1821, 40)\n"
          ]
        }
      ],
      "source": [
        "# This preprocessor function makes use of the tf.keras tokenizer\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Build vocabulary from training text data\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# preprocessor tokenizes words and makes sure all documents have the same length\n",
        "def preprocessor(data, maxlen=40, max_words=10000):\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    return X\n",
        "\n",
        "print(preprocessor(X_train).shape)\n",
        "print(preprocessor(X_test).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52kECL43b-O"
      },
      "source": [
        "## Fit model on preprocessed data and save preprocessor function and model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model1: An Embedding layer and three LSTM layers (Use an Embedding layer and LSTM layers in at least one model)"
      ],
      "metadata": {
        "id": "U0QD685CPXTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(10000, 16, input_length=40))\n",
        "model1.add(LSTM(32, return_sequences=True, dropout=0.2))\n",
        "model1.add(LSTM(32, return_sequences=True, dropout=0.2))\n",
        "model1.add(LSTM(32, dropout=0.2))\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model1.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=1,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Z4T2JBQV00",
        "outputId": "4161cecf-fc0e-4561-b3a4-3c65ffb1b835"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "173/173 [==============================] - 18s 60ms/step - loss: 0.6541 - acc: 0.6230 - val_loss: 0.7880 - val_acc: 0.3432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save preprocessor function to local \"preprocessor.zip\" file"
      ],
      "metadata": {
        "id": "x-i5wm5rWh4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aimodelshare as ai\n",
        "ai.export_preprocessor(preprocessor,\"\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLP_fNfHWi63",
        "outputId": "5d8e753c-1154-4eb4-ba9a-9e3ce729602e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your preprocessor is now saved to 'preprocessor.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model to local \".onnx\" file"
      ],
      "metadata": {
        "id": "OfMNH3HlXP5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model1 to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model1 = model_to_onnx(model1, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model1.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model1.SerializeToString())"
      ],
      "metadata": {
        "id": "Q1usEbMsSefc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate predictions from X_test data and submit model to competition\n"
      ],
      "metadata": {
        "id": "d9CQR8F1XYOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "\n",
        "from aimodelshare.aws import set_credentials\n",
        "    \n",
        "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
        "\n",
        "set_credentials(apiurl=apiurl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC9HVa5EXdbg",
        "outputId": "0373a618-dd3b-4cea-952f-2681d28620e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Modelshare Username:··········\n",
            "AI Modelshare Password:··········\n",
            "AI Model Share login credentials set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate Competition\n",
        "\n",
        "mycompetition= ai.Competition(apiurl)"
      ],
      "metadata": {
        "id": "Wo6JucfbS50I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit Model 1"
      ],
      "metadata": {
        "id": "bsknAzTzq7Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit Model 1: \n",
        "\n",
        "#-- Generate predicted y values (Model 1)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model1.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model1.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels,\n",
        "                           custom_metadata={\"team\":\"6\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOwgVlHySk4h",
        "outputId": "84ba02f4-5449-4cdd-b5c4-d329bd990046"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 1s 18ms/step\n",
            "Insert search tags to help users find your model (optional): Bob model 1\n",
            "Provide any useful notes about your model (optional): Bob model 1\n",
            "\n",
            "Your model has been submitted as model version 93\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model2: An Embedding layer and two Conv1d layers (Use an Embedding layer and Conv1d layers in at least one model)"
      ],
      "metadata": {
        "id": "XbMbwQdWYRQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCbBf8j9ClYl",
        "outputId": "afef329d-52d7-4260-e4a8-7b949133c183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 40, 16)            160000    \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 39, 64)            2112      \n",
            "                                                                 \n",
            " max_pooling1d_6 (MaxPooling  (None, 19, 64)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 8, 256)            65792     \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPooling  (None, 4, 256)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               262400    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 490,818\n",
            "Trainable params: 490,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 4s 17ms/step - loss: 0.6357 - acc: 0.6400 - val_loss: 0.7484 - val_acc: 0.5217\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 3s 16ms/step - loss: 0.3638 - acc: 0.8336 - val_loss: 0.7575 - val_acc: 0.6611\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.1406 - acc: 0.9440 - val_loss: 1.1081 - val_acc: 0.6113\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 3s 18ms/step - loss: 0.0583 - acc: 0.9760 - val_loss: 1.1632 - val_acc: 0.6842\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 4s 22ms/step - loss: 0.0364 - acc: 0.9818 - val_loss: 2.7812 - val_acc: 0.4617\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 4s 23ms/step - loss: 0.0313 - acc: 0.9850 - val_loss: 1.5275 - val_acc: 0.6640\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 3s 16ms/step - loss: 0.0250 - acc: 0.9872 - val_loss: 1.7872 - val_acc: 0.6662\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 3s 14ms/step - loss: 0.0209 - acc: 0.9890 - val_loss: 1.9886 - val_acc: 0.6322\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.0205 - acc: 0.9886 - val_loss: 2.1826 - val_acc: 0.6301\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 3s 16ms/step - loss: 0.0262 - acc: 0.9870 - val_loss: 3.9154 - val_acc: 0.5318\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Embedding(10000, 16, input_length=40))\n",
        "\n",
        "model2.add(Conv1D(64, kernel_size=2, strides=1))\n",
        "model2.add(MaxPooling1D(2))\n",
        "\n",
        "model2.add(Conv1D(256, kernel_size=4, strides=2))\n",
        "model2.add(MaxPooling1D(2))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(loss=\"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"acc\"])\n",
        "\n",
        "history = model2.fit(preprocessor(X_train), y_train,\n",
        "                    epochs = 10, \n",
        "                    batch_size = 32,\n",
        "                    validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model to local \".onnx\" file"
      ],
      "metadata": {
        "id": "yDYyQ71AplHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model2 to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model2 = model_to_onnx(model2, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model2.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model1.SerializeToString())"
      ],
      "metadata": {
        "id": "eV81HlVxpkOM"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit Model 2"
      ],
      "metadata": {
        "id": "LUujRjMMrCBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ql4wksyEUnP",
        "outputId": "936068f1-4c87-41e7-ab16-2adfae66cc82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 4ms/step\n",
            "Insert search tags to help users find your model (optional): Bob model 2\n",
            "Provide any useful notes about your model (optional): Bob model 2\n",
            "\n",
            "Your model has been submitted as model version 99\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "#Submit Model 2: \n",
        "\n",
        "#-- Generate predicted y values (Model 2)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model2.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model2.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels,\n",
        "                          custom_metadata={\"team\":\"6\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model3: Transfer learning with glove embeddings (Use transfer learning with glove embeddings for at least one of these models)"
      ],
      "metadata": {
        "id": "JvNivA72p-Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Glove embedding matrix weights (Might take 10 mins or so!)\n",
        "! wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAl2PhuAreuh",
        "outputId": "ef30c65d-8930-427c-e3f7-9fa73abd1aa4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-13 22:13:05--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-04-13 22:13:06--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2023-04-13 22:13:06--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.11MB/s    in 2m 40s  \n",
            "\n",
            "2023-04-13 22:15:46 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip glove.6B.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVPb63AUrgZy",
        "outputId": "1c1e982b-fa55-4d85-acec-06e03b2795ab"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Extract embedding data for 100 feature embedding matrix\n",
        "glove_dir = os.getcwd()\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA7K-n7VsPVB",
        "outputId": "0d78eb9e-0942-46a4-d3b3-a590abc97fee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400001 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "# Build embedding matrix\n",
        "embedding_dim = 100 # change if you use txt files using larger number of features\n",
        "\n",
        "embedding_matrix = np.zeros((10000, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < 10000:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "DeCtNVl3sc7b"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up model3 and import Glove weights to Embedding layer:\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(10000, 100, input_length=40))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(32, activation='relu'))\n",
        "model3.add(Dense(32, activation='relu'))\n",
        "model3.add(Dense(2, activation='softmax'))\n",
        "model3.summary()\n",
        "\n",
        "# Add weights in same manner as transfer learning and turn of trainable option before fitting model to freeze weights.\n",
        "model3.layers[0].set_weights([embedding_matrix])\n",
        "model3.layers[0].trainable = False\n",
        "\n",
        "model3.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model3.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htenOTvZs76N",
        "outputId": "c8bac2ed-e0a6-4b52-8683-099c16bc9f50"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_15 (Embedding)    (None, 40, 100)           1000000   \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 32)                128032    \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,129,154\n",
            "Trainable params: 1,129,154\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 2s 7ms/step - loss: 0.6263 - acc: 0.6404 - val_loss: 0.6606 - val_acc: 0.6481\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.5077 - acc: 0.7417 - val_loss: 0.9255 - val_acc: 0.4978\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.4168 - acc: 0.8029 - val_loss: 0.4878 - val_acc: 0.7926\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.3318 - acc: 0.8501 - val_loss: 0.6741 - val_acc: 0.7262\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.2520 - acc: 0.8972 - val_loss: 1.0434 - val_acc: 0.5889\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.1813 - acc: 0.9276 - val_loss: 0.7411 - val_acc: 0.7449\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.1229 - acc: 0.9545 - val_loss: 1.2392 - val_acc: 0.6669\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.0816 - acc: 0.9738 - val_loss: 1.1022 - val_acc: 0.7153\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.0517 - acc: 0.9832 - val_loss: 1.7632 - val_acc: 0.6590\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.0351 - acc: 0.9893 - val_loss: 1.9467 - val_acc: 0.6366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model to local \".onnx\" file"
      ],
      "metadata": {
        "id": "XfTl6NUcz4cD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "aIdmSpYVPYAw"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model3 = model_to_onnx(model3, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model3.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model3.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit Model 3"
      ],
      "metadata": {
        "id": "_Mv-syvRvIKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nszPPrfwPlUk",
        "outputId": "375afb62-9faa-4d78-d783-5019b150baa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 5ms/step\n",
            "Insert search tags to help users find your model (optional): Bob Model 3\n",
            "Provide any useful notes about your model (optional): Bob Model 3\n",
            "\n",
            "Your model has been submitted as model version 112\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "#Submit Model 3: \n",
        "\n",
        "#-- Generate predicted y values (Model 3)\n",
        "prediction_column_index=model3.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 3 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model3.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels,\n",
        "                           custom_metadata={\"team\":\"6\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "qLl7yLpVEx26",
        "outputId": "a35845c3-9467-4f61-d1a8-37e79a548fe3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_6eee9 caption {\n",
              "  color: black;\n",
              "  font-size: 18px;\n",
              "}\n",
              "#T_6eee9_row0_col0, #T_6eee9_row0_col3, #T_6eee9_row0_col6 {\n",
              "  background: #b3e2cd;\n",
              "  color: black;\n",
              "  color: black;\n",
              "}\n",
              "#T_6eee9_row0_col1, #T_6eee9_row0_col2, #T_6eee9_row0_col4, #T_6eee9_row0_col5, #T_6eee9_row0_col7, #T_6eee9_row0_col8, #T_6eee9_row1_col1, #T_6eee9_row1_col2, #T_6eee9_row1_col4, #T_6eee9_row1_col5, #T_6eee9_row1_col7, #T_6eee9_row1_col8, #T_6eee9_row2_col1, #T_6eee9_row2_col2, #T_6eee9_row2_col4, #T_6eee9_row2_col5, #T_6eee9_row2_col7, #T_6eee9_row2_col8, #T_6eee9_row3_col0, #T_6eee9_row3_col1, #T_6eee9_row3_col2, #T_6eee9_row3_col4, #T_6eee9_row3_col5, #T_6eee9_row3_col7, #T_6eee9_row3_col8, #T_6eee9_row4_col0, #T_6eee9_row4_col1, #T_6eee9_row4_col2, #T_6eee9_row4_col4, #T_6eee9_row4_col5, #T_6eee9_row4_col6, #T_6eee9_row4_col7, #T_6eee9_row4_col8 {\n",
              "  background: white;\n",
              "  color: black;\n",
              "  color: black;\n",
              "}\n",
              "#T_6eee9_row1_col0, #T_6eee9_row2_col6, #T_6eee9_row3_col3 {\n",
              "  background: nan;\n",
              "  color: black;\n",
              "  color: black;\n",
              "}\n",
              "#T_6eee9_row1_col3, #T_6eee9_row1_col6, #T_6eee9_row2_col3 {\n",
              "  background: #f1e2cc;\n",
              "  color: black;\n",
              "  color: black;\n",
              "}\n",
              "#T_6eee9_row2_col0, #T_6eee9_row3_col6, #T_6eee9_row4_col3 {\n",
              "  background: #fff2ae;\n",
              "  color: black;\n",
              "  color: black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_6eee9\">\n",
              "  <caption>Model type: Neural Network</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_6eee9_level0_col0\" class=\"col_heading level0 col0\" >Model_1_Layer</th>\n",
              "      <th id=\"T_6eee9_level0_col1\" class=\"col_heading level0 col1\" >Model_1_Shape</th>\n",
              "      <th id=\"T_6eee9_level0_col2\" class=\"col_heading level0 col2\" >Model_1_Params</th>\n",
              "      <th id=\"T_6eee9_level0_col3\" class=\"col_heading level0 col3\" >Model_2_Layer</th>\n",
              "      <th id=\"T_6eee9_level0_col4\" class=\"col_heading level0 col4\" >Model_2_Shape</th>\n",
              "      <th id=\"T_6eee9_level0_col5\" class=\"col_heading level0 col5\" >Model_2_Params</th>\n",
              "      <th id=\"T_6eee9_level0_col6\" class=\"col_heading level0 col6\" >Model_3_Layer</th>\n",
              "      <th id=\"T_6eee9_level0_col7\" class=\"col_heading level0 col7\" >Model_3_Shape</th>\n",
              "      <th id=\"T_6eee9_level0_col8\" class=\"col_heading level0 col8\" >Model_3_Params</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_6eee9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_6eee9_row0_col0\" class=\"data row0 col0\" >Embedding</td>\n",
              "      <td id=\"T_6eee9_row0_col1\" class=\"data row0 col1\" >[None, 40, 16]</td>\n",
              "      <td id=\"T_6eee9_row0_col2\" class=\"data row0 col2\" >160000.000000</td>\n",
              "      <td id=\"T_6eee9_row0_col3\" class=\"data row0 col3\" >Embedding</td>\n",
              "      <td id=\"T_6eee9_row0_col4\" class=\"data row0 col4\" >[None, 40, 16]</td>\n",
              "      <td id=\"T_6eee9_row0_col5\" class=\"data row0 col5\" >160000</td>\n",
              "      <td id=\"T_6eee9_row0_col6\" class=\"data row0 col6\" >Embedding</td>\n",
              "      <td id=\"T_6eee9_row0_col7\" class=\"data row0 col7\" >[None, 40, 16]</td>\n",
              "      <td id=\"T_6eee9_row0_col8\" class=\"data row0 col8\" >160000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6eee9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_6eee9_row1_col0\" class=\"data row1 col0\" >Flatten</td>\n",
              "      <td id=\"T_6eee9_row1_col1\" class=\"data row1 col1\" >[None, 640]</td>\n",
              "      <td id=\"T_6eee9_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "      <td id=\"T_6eee9_row1_col3\" class=\"data row1 col3\" >LSTM</td>\n",
              "      <td id=\"T_6eee9_row1_col4\" class=\"data row1 col4\" >[None, 40, 32]</td>\n",
              "      <td id=\"T_6eee9_row1_col5\" class=\"data row1 col5\" >6272</td>\n",
              "      <td id=\"T_6eee9_row1_col6\" class=\"data row1 col6\" >LSTM</td>\n",
              "      <td id=\"T_6eee9_row1_col7\" class=\"data row1 col7\" >[None, 40, 256]</td>\n",
              "      <td id=\"T_6eee9_row1_col8\" class=\"data row1 col8\" >279552.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6eee9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_6eee9_row2_col0\" class=\"data row2 col0\" >Dense</td>\n",
              "      <td id=\"T_6eee9_row2_col1\" class=\"data row2 col1\" >[None, 2]</td>\n",
              "      <td id=\"T_6eee9_row2_col2\" class=\"data row2 col2\" >1282.000000</td>\n",
              "      <td id=\"T_6eee9_row2_col3\" class=\"data row2 col3\" >LSTM</td>\n",
              "      <td id=\"T_6eee9_row2_col4\" class=\"data row2 col4\" >[None, 32]</td>\n",
              "      <td id=\"T_6eee9_row2_col5\" class=\"data row2 col5\" >8320</td>\n",
              "      <td id=\"T_6eee9_row2_col6\" class=\"data row2 col6\" >Flatten</td>\n",
              "      <td id=\"T_6eee9_row2_col7\" class=\"data row2 col7\" >[None, 10240]</td>\n",
              "      <td id=\"T_6eee9_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6eee9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_6eee9_row3_col0\" class=\"data row3 col0\" >None</td>\n",
              "      <td id=\"T_6eee9_row3_col1\" class=\"data row3 col1\" >None</td>\n",
              "      <td id=\"T_6eee9_row3_col2\" class=\"data row3 col2\" >nan</td>\n",
              "      <td id=\"T_6eee9_row3_col3\" class=\"data row3 col3\" >Flatten</td>\n",
              "      <td id=\"T_6eee9_row3_col4\" class=\"data row3 col4\" >[None, 32]</td>\n",
              "      <td id=\"T_6eee9_row3_col5\" class=\"data row3 col5\" >0</td>\n",
              "      <td id=\"T_6eee9_row3_col6\" class=\"data row3 col6\" >Dense</td>\n",
              "      <td id=\"T_6eee9_row3_col7\" class=\"data row3 col7\" >[None, 2]</td>\n",
              "      <td id=\"T_6eee9_row3_col8\" class=\"data row3 col8\" >20482.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_6eee9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_6eee9_row4_col0\" class=\"data row4 col0\" >None</td>\n",
              "      <td id=\"T_6eee9_row4_col1\" class=\"data row4 col1\" >None</td>\n",
              "      <td id=\"T_6eee9_row4_col2\" class=\"data row4 col2\" >nan</td>\n",
              "      <td id=\"T_6eee9_row4_col3\" class=\"data row4 col3\" >Dense</td>\n",
              "      <td id=\"T_6eee9_row4_col4\" class=\"data row4 col4\" >[None, 2]</td>\n",
              "      <td id=\"T_6eee9_row4_col5\" class=\"data row4 col5\" >66</td>\n",
              "      <td id=\"T_6eee9_row4_col6\" class=\"data row4 col6\" >None</td>\n",
              "      <td id=\"T_6eee9_row4_col7\" class=\"data row4 col7\" >None</td>\n",
              "      <td id=\"T_6eee9_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Compare model 1 2 3\n",
        "data=mycompetition.compare_models([1, 2, 3], verbose=1)\n",
        "mycompetition.stylize_compare(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discuss which models performed better and point out relevant hyper-parameter values for successful models."
      ],
      "metadata": {
        "id": "izMkoY-HvvsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model I have is model 3 (Transfer learning with glove embeddings), the train accuray is incredibly high, and the test accuracy is around 0.70. The relevant hyper-paramet values for this successful model is\n",
        "\n",
        " Embedding layer parameters:\n",
        "\n",
        "*  input_dim: 10000, which is the size of the vocabulary of the input data.\n",
        "*  output_dim: 100, which is the dimensionality of the output space.\n",
        "*  input_length: 40, which is the length of the input sequences.\n",
        "\n",
        "Dense layer parameters:\n",
        "\n",
        "\n",
        "*  units: 32, which is the number of output units in the layer.\n",
        "*  activation: 'relu', which is the activation function used by the layer.\n",
        "\n",
        "Dense layer parameters:\n",
        "\n",
        "\n",
        "*  units: 32, which is the number of output units in the layer.\n",
        "*  activation: 'relu', which is the activation function used by the layer.\n",
        "\n",
        "Dense layer parameters:\n",
        "\n",
        "\n",
        "*   units: 2, which is the number of output units in the layer.\n",
        "*   activation: 'softmax', which is the activation function used by the layer.\n",
        "*   epochs: 10, which is the number of times the model will iterate over the entire training dataset.\n",
        "*  batch_size: 32, which is the number of samples processed before the model is updated."
      ],
      "metadata": {
        "id": "FkYyxUVav4Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After talking with my teammates, I have a new model which has a higher test accuracy which I decrease one dense layer, and change the other layer's unit to 256."
      ],
      "metadata": {
        "id": "oVYH_HUzyehw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model4: "
      ],
      "metadata": {
        "id": "w9E7p8Xhyooc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = Sequential()\n",
        "model4.add(Embedding(10000, 100, input_length=40))\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(256, activation='relu'))\n",
        "model4.add(Dense(2, activation='softmax'))\n",
        "model4.summary()\n",
        "\n",
        "# Add weights in same manner as transfer learning and turn of trainable option before fitting model to freeze weights.\n",
        "model4.layers[0].set_weights([embedding_matrix])\n",
        "model4.layers[0].trainable = False\n",
        "\n",
        "model4.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model4.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO3BYu7Owidy",
        "outputId": "9fd1d1b3-fad5-46d4-af88-cdf542f4deba"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_16 (Embedding)    (None, 40, 100)           1000000   \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 4000)              0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 256)               1024256   \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,024,770\n",
            "Trainable params: 2,024,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 3s 15ms/step - loss: 0.6337 - acc: 0.6478 - val_loss: 0.8455 - val_acc: 0.4928\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 4s 23ms/step - loss: 0.4832 - acc: 0.7639 - val_loss: 0.6206 - val_acc: 0.6994\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 4s 24ms/step - loss: 0.3522 - acc: 0.8412 - val_loss: 0.6261 - val_acc: 0.7355\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 3s 19ms/step - loss: 0.2332 - acc: 0.9113 - val_loss: 0.7247 - val_acc: 0.6980\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 2s 14ms/step - loss: 0.1364 - acc: 0.9572 - val_loss: 0.8246 - val_acc: 0.7095\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 2s 13ms/step - loss: 0.0688 - acc: 0.9827 - val_loss: 0.9054 - val_acc: 0.7124\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 2s 14ms/step - loss: 0.0375 - acc: 0.9913 - val_loss: 1.5016 - val_acc: 0.6257\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 2s 14ms/step - loss: 0.0226 - acc: 0.9944 - val_loss: 1.4401 - val_acc: 0.6553\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 4s 20ms/step - loss: 0.0132 - acc: 0.9975 - val_loss: 1.5094 - val_acc: 0.6756\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 4s 21ms/step - loss: 0.0084 - acc: 0.9977 - val_loss: 1.8286 - val_acc: 0.6517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model to local \".onnx\" file"
      ],
      "metadata": {
        "id": "FWkq4dL0z7Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model4 = model_to_onnx(model4, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model4.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model4.SerializeToString())"
      ],
      "metadata": {
        "id": "Rs6pDmkHzMCg"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit Model 4"
      ],
      "metadata": {
        "id": "cMY3ZO-vzVDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit Model 4: \n",
        "\n",
        "#-- Generate predicted y values (Model 4)\n",
        "prediction_column_index=model4.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 4 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model4.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels,\n",
        "                           custom_metadata={\"team\":\"6\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nesBDPIEzQJg",
        "outputId": "8d37096b-c956-4bf1-c6bd-f07b7f084522"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 7ms/step\n",
            "Insert search tags to help users find your model (optional): Bob Model 4\n",
            "Provide any useful notes about your model (optional): Bob Model 4\n",
            "\n",
            "Your model has been submitted as model version 113\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discuss which models you tried and which models performed better and point out relevant hyper-parameter values for successful models."
      ],
      "metadata": {
        "id": "NtvmZtYoyHbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the best model I have is model 4 (Transfer learning with glove embeddings), the train accuray is also incredibly high, and the test accuracy is around 0.72. The relevant hyper-paramet values for this successful model is\n",
        "\n",
        " Embedding layer parameters:\n",
        "\n",
        "*  input_dim: 10000, which is the size of the vocabulary of the input data.\n",
        "*  output_dim: 100, which is the dimensionality of the output space.\n",
        "*  input_length: 40, which is the length of the input sequences.\n",
        "\n",
        "Dense layer parameters:\n",
        "\n",
        "\n",
        "*  units: 256, which is the number of output units in the layer.\n",
        "*  activation: 'relu', which is the activation function used by the layer.\n",
        "\n",
        "Dense layer parameters:\n",
        "\n",
        "\n",
        "*   units: 2, which is the number of output units in the layer.\n",
        "*   activation: 'softmax', which is the activation function used by the layer.\n",
        "*   epochs: 10, which is the number of times the model will iterate over the entire training dataset.\n",
        "*  batch_size: 32, which is the number of samples processed before the model is updated."
      ],
      "metadata": {
        "id": "WCx1siAa30DR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JXxGTgJz152A",
        "7JOeE5bPF2lg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}